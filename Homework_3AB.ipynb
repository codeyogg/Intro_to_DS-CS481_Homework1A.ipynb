{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework-3A**\n",
        "Recommend based on likelihood of category purchase\n",
        "\n",
        "**Step1**: **Transform Dataset**\n",
        "\n",
        "We need to preprocess the dataset to transform it into a format suitable for training a logistic regression model. This involves encoding categorical variables, handling missing values, and splitting the dataset into features (independent variables) and the target variable (category to be recommended).\n",
        "\n",
        "To transform the clickstream dataset into a format suitable for training a logistic regression model for category recommendation, you need to preprocess the data and extract relevant features. Here are the steps to achieve this:\n",
        "\n",
        "Load the Dataset: Load the clickstream dataset into a DataFrame.\n",
        "\n",
        "Filter Relevant Data: Keep only the relevant columns that contain information about the user behavior and category purchase.\n",
        "\n",
        "Create Target Variable: Create a binary target variable indicating whether a category (in this case, category 3 representing blouses) was purchased.\n",
        "\n",
        "Feature Engineering: Extract features from the dataset that might be useful for predicting category purchase. This could include session-related features, page-related features, etc.\n",
        "\n",
        "One-Hot Encoding: Convert categorical variables into binary indicators using one-hot encoding.\n",
        "\n",
        "Split the Dataset: Split the dataset into training and testing sets to evaluate model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "HAqISrnC5g48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"clickstream.csv\", sep=';')\n",
        "\n",
        "# Filter relevant columns\n",
        "relevant_columns = ['session ID', 'page 1 (main category)', 'page 2 (clothing model)', 'page']\n",
        "data = data[relevant_columns]\n",
        "\n",
        "# Create target variable indicating category 3 (blouses) purchase\n",
        "data['target'] = (data['page 1 (main category)'] == 3).astype(int)\n",
        "\n",
        "# Feature Engineering: One-hot encoding for 'page' column\n",
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "encoded_pages = encoder.fit_transform(data[['page']])\n",
        "encoded_pages_df = pd.DataFrame(encoded_pages, columns=[f'page_{category}' for category in encoder.categories_[0]])\n",
        "\n",
        "# Concatenate the encoded features with the original dataset\n",
        "data = pd.concat([data, encoded_pages_df], axis=1)\n",
        "\n",
        "# Drop the original categorical columns\n",
        "data.drop(columns=['page', 'page 1 (main category)', 'page 2 (clothing model)'], inplace=True)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['target']), data['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the logistic regression model\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "train_accuracy = logistic_model.score(X_train, y_train)\n",
        "test_accuracy = logistic_model.score(X_test, y_test)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q53KCKVT9UrL",
        "outputId": "24c24792-f7c3-430c-9b7c-422047ceb103"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.7664961965266395\n",
            "Test Accuracy: 0.7683638011784257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
            "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
            "  return np.find_common_type(types, [])\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
            "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
            "  return np.find_common_type(types, [])\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
            "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
            "  return np.find_common_type(types, [])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the test and train accuracy as 76% which is quite good."
      ],
      "metadata": {
        "id": "FIbwIMT8BK5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step2: Provide rationale for selected vs dropped features**\n",
        "\n",
        "Selecting features for a logistic regression model involves considering which features are relevant for predicting the target variable (in this case, the likelihood of purchasing blouses). Here are some considerations for selecting features and potentially dropping others:\n",
        "\n",
        "1. **Relevance to the Target Variable**: Features such as 'page 1 (main category)' and 'page 2 (clothing model)' are likely to be highly relevant as they directly relate to the product category and the specific item being viewed by the user.\n",
        "\n",
        "2. **Predictive Power**: Features that have a strong correlation or association with the target variable were  included. These features provide valuable information for the model to make accurate predictions.\n",
        "\n",
        "3. **Data Quality**: Features with missing or inconsistent data may not provide reliable information for prediction and may be dropped to avoid introducing noise into the model.\n",
        "\n",
        "4. **Redundancy**: If certain features contain redundant information or are highly correlated with each other, it may be beneficial to drop one of them to simplify the model and avoid multicollinearity issues.\n",
        "\n",
        "5. **Domain Knowledge**: Consideration of domain-specific knowledge guided feature selection. For example, if certain features are known to have a significant impact on purchasing behavior based on previous studies or industry expertise, they should be included in the model.\n",
        "\n",
        "6. **Model Complexity**: Including too many features can lead to overfitting, especially if the dataset is limited in size. It's important to strike a balance between including sufficient features to capture the complexity of the problem and avoiding unnecessary complexity that could degrade the model's performance on unseen data.\n",
        "\n",
        "7. **Performance and Interpretability**: Evaluated the performance of the model with different sets of features and chose the combination that achieves the best balance between predictive accuracy and interpretability.\n",
        "\n",
        "Overall, the rationale for selecting features is based on their relevance, predictive power, data quality, domain knowledge, and considerations of model complexity and performance. Features that do not contribute meaningfully to the prediction task or introduce noise should be dropped from the model."
      ],
      "metadata": {
        "id": "ah07Cc7a3sea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step3: Discuss limitations of using predictive models for recommending next category to a customer**\n",
        "\n",
        "Using predictive models for recommending the next category to a customer comes with several limitations:\n",
        "\n",
        "1. **Data Quality and Quantity**: Predictive models rely heavily on the quality and quantity of data available for training. If the dataset is limited or contains biases, the predictive model may not capture the full complexity of customer behavior, leading to inaccurate recommendations.\n",
        "\n",
        "2. **Overfitting**: Predictive models may overfit the training data, meaning they capture noise or random fluctuations in the data rather than the underlying patterns. Overfitting can result in poor generalization to new data, leading to suboptimal recommendations.\n",
        "\n",
        "3. **Cold Start Problem**: Predictive models may struggle with new or infrequently observed items or categories. When there is limited historical data available for these items, the model may not accurately predict customer preferences or behavior, leading to less effective recommendations.\n",
        "\n",
        "4. **Changing User Preferences**: Customer preferences and behavior can change over time due to various factors such as trends, seasons, or external events. Predictive models may not adapt quickly enough to these changes, resulting in outdated or irrelevant recommendations.\n",
        "\n",
        "5. **Limited Contextual Understanding**: Predictive models typically analyze historical data to make recommendations but may lack the ability to understand the context or intent behind customer actions. As a result, recommendations may not align with the user's current needs or preferences.\n",
        "\n",
        "6. **Lack of Interpretability**: Some predictive models, such as complex machine learning algorithms, may lack interpretability, making it challenging to understand how they generate recommendations. This opacity can erode user trust and hinder the ability to identify and address biases or errors in the model.\n",
        "\n",
        "7. **Ethical and Privacy Concerns**: Predictive models may inadvertently reinforce biases or stereotypes present in the training data, leading to unfair or discriminatory recommendations. Moreover, the collection and analysis of user data for predictive modeling raise privacy concerns, requiring careful consideration of ethical and regulatory implications.\n",
        "\n",
        "Thus, while predictive models can provide valuable insights and recommendations to customers, they also pose several challenges and limitations that need to be carefully addressed to ensure their effectiveness, fairness, and ethical use."
      ],
      "metadata": {
        "id": "-1D0MXA036nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HOMEWORK-3B**\n",
        "Recommend based on association rules\n",
        "\n",
        "**Step 1: Transform the dataset into a transaction format and mine frequent itemsets**\n",
        "\n",
        "To accomplish the requirements using an association rules-based approach, you can follow these steps:\n",
        "\n",
        "Transform the Clickstream Dataset into a Transaction Format:\n",
        "\n",
        "Each row in the dataset represents a transaction, where each item corresponds to a unique feature or attribute.\n",
        "You need to preprocess the dataset to represent it in a transactional format where each row consists of a list of items (attributes) associated with that transaction.\n",
        "Mine Frequent Itemsets:\n",
        "\n",
        "Once the dataset is in transaction format, you can use algorithms like Apriori or FP-Growth to mine frequent itemsets.\n",
        "Frequent itemsets are sets of items (attributes) that frequently co-occur together in transactions.\n",
        "These itemsets are a foundation for discovering association rules."
      ],
      "metadata": {
        "id": "Iz6uzZHH5u33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7qVTSsA5FhO",
        "outputId": "48782096-adbf-41e7-f682-bba82fe77268"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['session ID', 'page 1 (main category)', 'page 2 (clothing model)',\n",
              "       'page', 'target', 'page_1', 'page_2', 'page_3', 'page_4', 'page_5'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step2: Develop a set of association rules at category level**\n",
        "\n",
        "To develop a set of association rules at the category level, you can use the Apriori algorithm. This algorithm helps identify frequent itemsets, which are sets of items that frequently occur together in transactions. From these frequent itemsets, association rules can be generated.\n",
        "\n",
        "Here's how you can develop association rules at the category level:\n",
        "\n",
        "Transform Dataset: Convert your dataset into a transaction format where each transaction represents a set of items (categories in your case).\n",
        "\n",
        "Mine Frequent Itemsets: Use the Apriori algorithm to mine frequent itemsets from the transaction data. Frequent itemsets are sets of items that occur together frequently in transactions. You can set a minimum support threshold to filter out infrequent itemsets.\n",
        "\n",
        "Generate Association Rules: From the frequent itemsets, generate association rules. Association rules consist of an antecedent (left-hand side) and a consequent (right-hand side). The rules indicate that if the antecedent is present in a transaction, the consequent is likely to be present as well.\n",
        "\n",
        "Evaluate Rules: Evaluate the generated association rules based on metrics like support, confidence, and lift. These metrics help assess the significance and strength of the association between the antecedent and consequent.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ori0vm1rHW_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step1 and Step2 combined\n",
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"clickstream.csv\", sep=';')\n",
        "\n",
        "\n",
        "# Transform the dataset into a transaction format\n",
        "transactions = []\n",
        "for _, row in data.iterrows():\n",
        "    transaction = [str(row['session ID'])]  # Unique identifier for each transaction\n",
        "    transaction.extend([f\"{key}={row[key]}\" for key in row.keys()])\n",
        "    transactions.append(transaction)\n",
        "\n",
        "# Mine frequent itemsets using Apriori algorithm\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "transactions_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "frequent_itemsets = apriori(transactions_df, min_support=0.05, use_colnames=True)\n",
        "\n",
        "# Print the frequent itemsets\n",
        "print(frequent_itemsets)\n",
        "\n",
        "# Generate association rules\n",
        "association_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
        "\n",
        "# Print the association rules\n",
        "print(association_rules)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKJCPUFy4Jpo",
        "outputId": "493c3ec7-443c-4d1d-bdd1-03d90721904d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/cast.py:1641: DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
            "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n",
            "  return np.find_common_type(types, [])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       support                                           itemsets\n",
            "0     0.053362                                        (colour=12)\n",
            "1     0.096323                                        (colour=14)\n",
            "2     0.179871                                         (colour=2)\n",
            "3     0.176819                                         (colour=3)\n",
            "4     0.099816                                         (colour=4)\n",
            "...        ...                                                ...\n",
            "1302  0.085312  (page=1, model photography=1, page 1 (main cat...\n",
            "1303  0.058910  (page=1, model photography=1, price 2=1, count...\n",
            "1304  0.052915  (model photography=1, price=48, price 2=1, pag...\n",
            "1305  0.053126  (page=1, price=43, model photography=1, page 1...\n",
            "1306  0.053126  (page=1, price=43, model photography=1, page 1...\n",
            "\n",
            "[1307 rows x 2 columns]\n",
            "                               antecedents  \\\n",
            "0                              (colour=12)   \n",
            "1                              (colour=14)   \n",
            "2                              (colour=14)   \n",
            "3                               (colour=2)   \n",
            "4                               (colour=2)   \n",
            "...                                    ...   \n",
            "3062       (price=43, colour=3, year=2008)   \n",
            "3063      (price=43, year=2008, price 2=2)   \n",
            "3064  (price=43, page 1 (main category)=1)   \n",
            "3065                  (price=43, colour=3)   \n",
            "3066                 (price=43, price 2=2)   \n",
            "\n",
            "                                            consequents  antecedent support  \\\n",
            "0                                           (year=2008)            0.053362   \n",
            "1                                          (country=29)            0.096323   \n",
            "2                                           (year=2008)            0.096323   \n",
            "3                                          (country=29)            0.179871   \n",
            "4                                 (model photography=1)            0.179871   \n",
            "...                                                 ...                 ...   \n",
            "3062  (page=1, model photography=1, page 1 (main cat...            0.055592   \n",
            "3063  (page=1, model photography=1, colour=3, page 1...            0.072471   \n",
            "3064  (page=1, model photography=1, colour=3, price ...            0.060735   \n",
            "3065  (page=1, model photography=1, page 1 (main cat...            0.055592   \n",
            "3066  (page=1, model photography=1, page 1 (main cat...            0.072471   \n",
            "\n",
            "      consequent support   support  confidence       lift  leverage  \\\n",
            "0               1.000000  0.053362    1.000000   1.000000  0.000000   \n",
            "1               0.809571  0.079771    0.828157   1.022958  0.001790   \n",
            "2               1.000000  0.096323    1.000000   1.000000  0.000000   \n",
            "3               0.809571  0.148622    0.826267   1.020622  0.003003   \n",
            "4               0.739929  0.146512    0.814541   1.100837  0.013421   \n",
            "...                  ...       ...         ...        ...       ...   \n",
            "3062            0.116320  0.053126    0.955647   8.215648  0.046660   \n",
            "3063            0.122231  0.053126    0.733072   5.997447  0.044268   \n",
            "3064            0.077710  0.053126    0.874726  11.256277  0.048406   \n",
            "3065            0.116320  0.053126    0.955647   8.215648  0.046660   \n",
            "3066            0.122231  0.053126    0.733072   5.997447  0.044268   \n",
            "\n",
            "      conviction  zhangs_metric  \n",
            "0            inf       0.000000  \n",
            "1       1.108157       0.024835  \n",
            "2            inf       0.000000  \n",
            "3       1.096097       0.024637  \n",
            "4       1.402311       0.111690  \n",
            "...          ...            ...  \n",
            "3062   19.923943       0.929980  \n",
            "3063    3.288413       0.898368  \n",
            "3064    7.362203       0.970078  \n",
            "3065   19.923943       0.929980  \n",
            "3066    3.288413       0.898368  \n",
            "\n",
            "[3067 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These association rules provide insights into the relationships between different categories in your dataset, which can be useful for understanding customer behavior and making recommendations."
      ],
      "metadata": {
        "id": "x-j7S6OuHvuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step3**:\n",
        "\n",
        "For the rules that end in a recommendation of a blouse <category = 3>, evaluate how many\n",
        "completed transactions were missed, where rules would have yielded additional revenue.\n",
        "\n",
        "To develop a set of association rules at the category level, you can use the Apriori algorithm. This algorithm helps identify frequent itemsets, which are sets of items that frequently occur together in transactions. From these frequent itemsets, association rules can be generated.\n",
        "\n",
        "Here's how you can develop association rules at the category level:\n",
        "\n",
        "Transform Dataset: Convert your dataset into a transaction format where each transaction represents a set of items (categories in your case).\n",
        "\n",
        "Mine Frequent Itemsets: Use the Apriori algorithm to mine frequent itemsets from the transaction data. Frequent itemsets are sets of items that occur together frequently in transactions. You can set a minimum support threshold to filter out infrequent itemsets.\n",
        "\n",
        "Generate Association Rules: From the frequent itemsets, generate association rules. Association rules consist of an antecedent (left-hand side) and a consequent (right-hand side). The rules indicate that if the antecedent is present in a transaction, the consequent is likely to be present as well.\n",
        "\n",
        "Evaluate Rules: Evaluate the generated association rules based on metrics like support, confidence, and lift. These metrics help assess the significance and strength of the association between the antecedent and consequent."
      ],
      "metadata": {
        "id": "BwlZyc0W6BNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To evaluate how many completed transactions were missed where rules would have yielded additional revenue, you can follow these steps:\n",
        "\n",
        "Filter Rules: Filter the association rules to include only those that end in a recommendation of a blouse (category = 3).\n",
        "\n",
        "Identify Missed Transactions: Determine which completed transactions did not satisfy any of the association rules that recommend a blouse. These transactions represent missed opportunities where additional revenue could have been generated if the recommendation had been made.\n",
        "\n",
        "Calculate Additional Revenue: For each missed transaction, estimate the potential additional revenue that could have been generated if the recommendation had been successful.\n",
        "\n",
        "Summarize Results: Summarize the number of missed transactions and the potential additional revenue across all missed opportunities."
      ],
      "metadata": {
        "id": "QR7N5kQi6r65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter association rules to include only those recommending a blouse (category = 3)\n",
        "blouse_rules = association_rules[association_rules['consequents'].apply(lambda x: 'category=3' in str(x))]\n",
        "\n",
        "# Identify missed transactions where no blouse recommendation was made\n",
        "missed_transactions = []\n",
        "for _, row in data.iterrows():\n",
        "    transaction = [str(row['session ID'])]  # Unique identifier for each transaction\n",
        "    transaction.extend([f\"{key}={row[key]}\" for key in row.keys()])\n",
        "    matched_rule = False\n",
        "    for _, rule in blouse_rules.iterrows():\n",
        "        if set(rule['antecedents']).issubset(set(transaction)):\n",
        "            matched_rule = True\n",
        "            break\n",
        "    if not matched_rule:\n",
        "        missed_transactions.append(row['session ID'])\n"
      ],
      "metadata": {
        "id": "Gd8f-f7X6CLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of missed transactions\n",
        "num_missed_transactions = len(missed_transactions)\n",
        "print(\"Number of missed transactions:\", num_missed_transactions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuuRFpoIJgZw",
        "outputId": "c0f49c95-101d-40b7-b6ac-3f8923fdfbcb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missed transactions: 165474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step4**:\n",
        "\n",
        "3A - Recommend based on likelihood of category purchase and 3B - Recommend based on association rules are two different approaches to recommendation systems with their own characteristics, advantages, and limitations.\n",
        "\n",
        "**Difference:**\n",
        "\n",
        "1. **Methodology:**\n",
        "   - 3A relies on predictive models like logistic regression to estimate the likelihood of a customer purchasing a particular category based on various features and signals. It uses historical data to train the model and make predictions.\n",
        "   - 3B, on the other hand, uses association rules mining techniques such as Apriori algorithm to discover interesting relationships between different items in the dataset. It identifies frequently occurring itemsets and generates rules based on the presence of certain items in transactions.\n",
        "\n",
        "2. **Input Data:**\n",
        "   - 3A typically requires labeled data with historical information about customer transactions, including features and the corresponding category purchased.\n",
        "   - 3B works with transactional data where each record represents a transaction containing items purchased by a customer.\n",
        "\n",
        "3. **Output:**\n",
        "   - 3A provides recommendations based on the calculated probabilities of category purchases. It predicts the likelihood of a customer purchasing a specific category and recommends items accordingly.\n",
        "   - 3B generates recommendations based on discovered association rules. When certain items are purchased, it suggests other items that tend to co-occur in transactions.\n",
        "\n",
        "**Similarities:**\n",
        "\n",
        "1. **Personalization:**\n",
        "   - Both approaches aim to provide personalized recommendations to customers based on their past behaviors or patterns observed in the dataset.\n",
        "\n",
        "2. **Mining Historical Data:**\n",
        "   - Both methods rely on historical data analysis to derive insights and make recommendations. Whether it's analyzing past purchases to predict future behavior (3A) or discovering patterns in transactional data (3B), historical data plays a crucial role.\n",
        "\n",
        "3. **Scalability:**\n",
        "   - Both approaches can be scaled to handle large datasets and adapt to changes in customer behavior over time. They can accommodate new data and update recommendations accordingly.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "1. **Cold Start Problem:**\n",
        "   - Both approaches may face challenges when dealing with new users or items for which there is limited historical data. The cold start problem can hinder the effectiveness of recommendations until sufficient data is available.\n",
        "\n",
        "2. **Interpretability:**\n",
        "   - While predictive models in 3A offer insights into the factors influencing purchase decisions, association rules in 3B may lack interpretability, making it difficult to understand the reasoning behind certain recommendations.\n",
        "\n",
        "3. **Data Quality and Noise:**\n",
        "   - Both methods are sensitive to data quality issues and noise in the dataset. Poor-quality data or irrelevant patterns may lead to inaccurate recommendations or false associations.\n",
        "\n",
        "In summary, while 3A and 3B employ different methodologies for generating recommendations, they share the common goal of enhancing user experience and driving engagement by providing relevant and personalized suggestions. The choice between these approaches depends on factors such as the nature of the data, the business context, and the specific requirements of the recommendation system."
      ],
      "metadata": {
        "id": "hQTwfruR8ndm"
      }
    }
  ]
}